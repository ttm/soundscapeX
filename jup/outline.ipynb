{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# summary of what we are trying to do\n",
    "PLCA2D is not fast enough for interactive analysis of ecological soundscapes because the audio files are often lengthy.\n",
    "\n",
    "Thus, hopping that it will make processing faster, and/or aid the detection of the sources, we want to enable the user to select an except to detect the sources/patterns/codes, and then use one (or more) of the patterns to the whole soundfile or a longer excerpt. Then, e.g. an onset detection would provide the analyst with cues to where the events occur.\n",
    "\n",
    "### procedure\n",
    "The user:\n",
    "1. Selects an audio excerpt to find the patterns (or codes) of the different sonic sources, such as species.\n",
    "2. May hear each of the reconstructed patches (related to each detected pattern).\n",
    "3. Than asks for the interface to find the patterns along the whole file or an excerpt.\n",
    "\n",
    "### theoretical sketch\n",
    "This is a very preliminary outline of what we are looking for.\n",
    "Using the notation in as in [1], something like:\n",
    "$$ V_1 = W_1 Z_1 H_1 $$\n",
    "$$ H_2^{-1} = V_2^{-1} W_1 Z_1 $$\n",
    "\n",
    "Ideally, using only part of $W_1$ and $Z_1$.\n",
    "Then use $H_2$, $W_1$ and $Z_1$ for reconstruction of the patches.\n",
    "\n",
    "\n",
    "[1] https://peerj.com/articles/2108/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bregman.suite import *\n",
    "import AudioSpectrumPatchApproximation as A\n",
    "import pylab as p\n",
    "from scipy.io import wavfile as w\n",
    "import urllib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getFile(url):\n",
    "    fname = url.split('/')[-1]\n",
    "    urllib.urlretrieve(url, fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "getFile('http://bbcsfx.acropolis.org.uk/assets/07070120.wav')\n",
    "getFile('https://github.com/ttm/soundscapeX/raw/master/exp2/forest1.wav')\n",
    "getFile('https://github.com/ttm/soundscapeX/raw/master/exp2/birds_.wav')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# birds_.wav is just an excerpt of forrest1.wav\n",
    "F_ = LogFrequencySpectrum('birds_.wav',nhop=1024, nfft=8192, wfft=4096, npo=24)\n",
    "\n",
    "s3 = A.SparseApproxSpectrumPLCA2D(patch_size=(12,8))\n",
    "s3.extract_codes(\n",
    "    F_, n_components=3, log_amplitude=True, alphaW=0.0,\n",
    "    alphaZ=0.0, alphaH=0.0, betaW=0.00, betaZ=0.001, betaH=0.00\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############\n",
    "# step 2: find the components of the complete audio using the frequency patterns found:\n",
    "F = LogFrequencySpectrum('forest1.wav',nhop=1024, nfft=8192, wfft=4096, npo=24)\n",
    "\n",
    "# we have F_.w and F_.z, we want to use them (or similar) to find F.h and thus the sparse approximation:\n",
    "# something like: SIPLCA2.reconstruct(F_.w, F_.z, F.h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15+"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
